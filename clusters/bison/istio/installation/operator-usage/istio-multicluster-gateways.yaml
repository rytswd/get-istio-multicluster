apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: bison-istio-multicluster-gateways
  annotations:
    # ArgoCD Wave setup for controlling the timing of sync.
    argocd.argoproj.io/sync-wave: "2"
spec:
  # Profile empty is used to create Gateways only.
  profile: empty

  components:
    egressGateways:
      - enabled: true
        name: bison-multicluster-egressgateway
        label:
          app: bison-multicluster-egressgateway
        # k8s:
        #   env:
        #     - name: ISTIO_META_REQUESTED_NETWORK_VIEW
        #       value: external

    ingressGateways:
      - enabled: true
        name: bison-multicluster-ingressgateway
        label:
          app: bison-multicluster-ingressgateway
          topology.istio.io/network: bison
        k8s:
          env:
            # sni-dnat adds the clusters required for AUTO_PASSTHROUGH mode
            - name: ISTIO_META_ROUTER_MODE
              value: "sni-dnat"
            # traffic through this gateway should be routed inside the network
            - name: ISTIO_META_REQUESTED_NETWORK_VIEW
              value: bison

          service:
            # This is assuming that you are using MetalLB with KinD. Your KinD
            # network may be differently set up, and in that case, you would
            # need to adjust this LB IP and also MetalLB IP ranges.
            # In real use cases, you will likely want to create an LB IP
            # beforehand, and use that IP here.
            loadBalancerIP: 172.18.102.150

            ports:
              - port: 80
                targetPort: 8080
                name: http2
              - port: 443
                targetPort: 8443
                name: https
              - port: 15443
                targetPort: 15443
                name: tls
                nodePort: 32022 # This is for KinD based testing. Make sure this doesn't clash with other mesh definitions.
              - name: status-port
                port: 15021
                targetPort: 15021
              - name: tls-istiod
                port: 15012
                targetPort: 15012
              - name: tls-webhook
                port: 15017
                targetPort: 15017

          # Affinity for IngressGateways, so that pods are separated into
          # different nodes.
          affinity:
            {}
            # For real use cases, it is probably better to have affinity setup.
            # This is commented out only for simplicity.
            # podAntiAffinity:
            #   requiredDuringSchedulingIgnoredDuringExecution:
            #     - labelSelector:
            #         matchLabels:
            #           app: bison-multicluster-ingressgateway
            #       topologyKey: kubernetes.io/hostname

          # HPA setup, which is commented out for local testing.
          # hpaSpec:
          #   maxReplicas: 4
          #   metrics:
          #     - resource:
          #         name: cpu
          #         targetAverageUtilization: 80
          #       type: Resource
          #   minReplicas: 2
          #   scaleTargetRef:
          #     apiVersion: apps/v1
          #     kind: Deployment
          #     name: bison-multicluster-ingressgateway

  values:
    global:
      meshID: get-istio-multicluster
      network: bison
      multiCluster:
        clusterName: bison
        # With the below flag commented out, you wouldn't get DR, GW and EF
        # created as a part of the spec. Because we use multiple Istio
        # IngressGateways, it is better to manage this separately. Those are
        # defined in traffic-management/multicluster directory.
        # enabled: true
